import gradio as gr
import re
import chromadb
import gradio as gr
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# ---------------- le mod√®le

COLLECTION_NAME = "la_belle_pizza_collection"
EMBEDDING_MODEL = "nomic-embed-text:v1.5"
LLM_MODEL = "llama3.1:8b"
CHROMA_PATH = "./data/chroma_db"

client = chromadb.PersistentClient(path=CHROMA_PATH)

ollama_embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

vectorstore = Chroma(
    client=client,
    collection_name=COLLECTION_NAME,
    embedding_function=ollama_embeddings,
)

retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={
        "k": 8  # Booommmmm ! On charge !
    }
)

llm = ChatOllama(model=LLM_MODEL)

template = """Tu es un assistant sp√©cialis√© dans les informations sur le menu du restaurant VAPIANO.

## R√àGLES IMPORTANTES
- R√©ponds UNIQUEMENT avec les informations pr√©sentes dans le contexte fourni
- Pour les allerg√®nes, sois TR√àS pr√©cis et mentionne TOUS les allerg√®nes list√©s
- Si une information n'est pas dans le contexte, dis "Je n'ai pas cette information dans ma base de donn√©es"
- Utilise les noms EXACTS des plats tels qu'ils apparaissent dans le contexte

## CONTEXTE FOURNI
{context}

## QUESTION DU CLIENT
{question}

## R√âPONSE
R√©ponds de mani√®re claire et pr√©cise en utilisant uniquement les informations du contexte ci-dessus."""

prompt = ChatPromptTemplate.from_template(template)

# rag_chain = (
#     {"context": retriever, "question": RunnablePassthrough()}
#     | prompt
#     | llm
#     | StrOutputParser()
# )


# ------------- Bon la j'ai trich√© c'est l'ia :


def extract_dish_name(question):
    """
    Extrait le nom du plat de la question
    """
    # Cherche des patterns comme "pizza X" ou "je voudrais X"
    patterns = [
        r'pizza\s+([A-Z\s_]+?)(?:\s*[.?!]|$)',
        r'prendre\s+(?:la\s+)?(?:pizza\s+)?([A-Z\s_]+?)(?:\s*[.?!]|$)',
        r'voudrais\s+(?:la\s+)?(?:pizza\s+)?([A-Z\s_]+?)(?:\s*[.?!]|$)',
    ]
    
    question_upper = question.upper()
    
    for pattern in patterns:
        match = re.search(pattern, question_upper)
        if match:
            dish_name = match.group(1).strip()
            # Nettoie le nom
            dish_name = re.sub(r'\s+', ' ', dish_name)
            return dish_name
    
    return None

def hybrid_search(question, k=8):
    """
    Recherche hybride qui combine recherche vectorielle et recherche par mots-cl√©s
    """
    print(f"\nüîç RECHERCHE HYBRIDE: {question}")
    
    # 1. Recherche vectorielle standard
    docs = vectorstore.similarity_search(question, k=k)
    
    # 2. Extraction du nom du plat
    dish_name = extract_dish_name(question)
    print(f"üçï Plat d√©tect√©: {dish_name}")
    
    # 3. Recherche sp√©cifique par nom de plat si d√©tect√©
    if dish_name:
        # Essaie plusieurs variantes du nom
        dish_variants = [
            dish_name,
            dish_name.replace(' ', '_'),
            dish_name.replace('_', ' '),
            dish_name.replace('DI BUFALA', 'DI_BUFALA'),
            dish_name.replace('DI_BUFALA', 'DI BUFALA')
        ]
        
        for variant in dish_variants:
            print(f"üîé Recherche variante: {variant}")
            variant_docs = vectorstore.similarity_search(
                f"allerg√®nes {variant}",
                k=3,
                filter={"type": "allergenes"}
            )
            
            # Ajoute les nouveaux documents
            for doc in variant_docs:
                if doc not in docs and variant.upper() in doc.page_content.upper():
                    docs.insert(0, doc)  # Ajoute en priorit√©
                    print(f"‚úÖ Document sp√©cifique trouv√© pour {variant}")
    
    # 4. Recherche sp√©cifique allerg√®nes si question d'allergie
    allergen_keywords = ['allergique', 'allergie', 'allerg√®ne', 'c√©leri', 'gluten', 'lait', '≈ìuf', 'soja']
    if any(keyword in question.lower() for keyword in allergen_keywords):
        print("üö® Recherche allerg√®nes activ√©e")
        
        allergen_docs = vectorstore.similarity_search(
            f"allerg√®nes pizza {question}",
            k=5,
            filter={"type": "allergenes"}
        )
        
        # Combine en √©vitant les doublons
        for doc in allergen_docs:
            if doc not in docs:
                docs.append(doc)
    
    # 5. Si c'est une question sur MARGHERITA DI BUFALA, force la r√©cup√©ration
    if 'margherita' in question.lower() and 'bufala' in question.lower():
        print("üéØ Recherche forc√©e MARGHERITA DI BUFALA")
        
        # R√©cup√®re TOUS les documents et filtre manuellement
        all_docs = vectorstore.similarity_search("MARGHERITA_DI_BUFALA allerg√®nes c√©leri", k=15)
        
        for doc in all_docs:
            if "MARGHERITA_DI_BUFALA" in doc.page_content and "C√âLERI" in doc.page_content:
                # Ajoute en premi√®re position si pas d√©j√† pr√©sent
                if doc not in docs:
                    docs.insert(0, doc)
                    print("‚úÖ Document MARGHERITA_DI_BUFALA forc√© en premi√®re position!")
                break
    
    # Limite √† k documents maximum
    docs = docs[:k]
    
    print(f"üìÑ {len(docs)} documents finaux s√©lectionn√©s:")
    for i, doc in enumerate(docs):
        metadata = doc.metadata
        content_preview = doc.page_content[:100].replace('\n', ' ')
        print(f"  {i+1}. {metadata.get('type', 'N/A')} ({metadata.get('source', 'N/A')}): {content_preview}...")
    
    return docs

def ask_rag(question):
    """
    Fonction RAG avec recherche hybride am√©lior√©e
    """
    try:
        if not question.strip():
            return "Veuillez poser une question."
        
        print(f"\n" + "="*60)
        print(f"QUESTION: {question}")
        
        # Recherche hybride
        docs = hybrid_search(question)
        
        # Formatage du contexte
        context_parts = []
        for doc in docs:
            metadata = doc.metadata
            source_info = f"[Source: {metadata.get('source', 'N/A')} - Type: {metadata.get('type', 'N/A')}]"
            context_parts.append(f"{source_info}\n{doc.page_content}\n")
        
        context = "\n".join(context_parts)
        
        # G√©n√©ration de la r√©ponse
        formatted_prompt = prompt.format(context=context, question=question)
        response = llm.invoke(formatted_prompt)
        
        if hasattr(response, 'content'):
            final_response = response.content
        else:
            final_response = str(response)
        
        print(f"‚úÖ R√âPONSE: {final_response[:200]}...")
        print("="*60)
        
        return final_response
    
    except Exception as e:
        error_msg = f"Erreur lors du traitement de votre question: {str(e)}"
        print(f"‚ùå {error_msg}")
        return error_msg

# ------------- l'interface de gradio

# def ask_rag(question):
#     try:
#         return rag_chain.invoke(question)
#     except Exception as e:
#         return f"Erreur : {str(e)}"

with gr.Blocks(title="Chatbot - La belle piza !") as demo:
    gr.Markdown("""
        **Vla la doc !**
        """)
    gr.File(value="./data/pdf/Menu.pdf", label="Menu", interactive=False)
    gr.File(value="./data/pdf/Liste_allergenes.pdf", label="Liste des allerg√®nes", interactive=False)
    gr.Markdown("Posez une question sur le menu ou les allerg√®nes.")
    gr.Markdown("ex : Bonjour, j'aime le fromage et les c√¢pres, as tu un conseil ?")
    gr.Markdown("ex : Bonjour, je voudrais prendre la pizza MARGHERITA DI BUFALA. Je suis allergique au c√©leri c'est un soucis ? ")

    with gr.Row():
        chat_input = gr.Textbox(label="Votre question", placeholder="Quels plats ne contiennent pas de gluten ?")
        chat_output = gr.Textbox(label="R√©ponse du chatbot")

    submit_btn = gr.Button("Envoyer")

    submit_btn.click(fn=ask_rag, inputs=chat_input, outputs=chat_output)


demo.launch(server_name="0.0.0.0", server_port=7860)