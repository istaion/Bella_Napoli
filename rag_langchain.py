import chromadb
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
import os 
os.environ["CHROMA_ENABLE_TELEMETRY"] = "false"
# --- 1. Configuration ---

# Le nom de la collection que nous avons cr√©√©e dans le script pr√©c√©dent.
COLLECTION_NAME = "la_belle_pizza_collection"
# Le mod√®le d'embedding (doit √™tre le m√™me que celui utilis√© pour la cr√©ation).
EMBEDDING_MODEL = "nomic-embed-text:v1.5"
# Le mod√®le de LLM √† utiliser pour la g√©n√©ration de la r√©ponse.
LLM_MODEL = "mistral:7b"

print("Initialisation de ChromaDB...")
# Cr√©e un client ChromaDB qui stockera les donn√©es sur le disque dans le dossier `chroma_db`
client = chromadb.PersistentClient(path="./data/chroma_db")

# --- 2. Initialisation des composants LangChain ---

print("Initialisation des composants LangChain...")

# Initialise le client Ollama pour les embeddings
ollama_embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

# Initialise le client ChromaDB pour se connecter √† la base de donn√©es existante.
vectorstore = Chroma(
    client=chromadb.PersistentClient(path="./data/chroma_db"),
    collection_name=COLLECTION_NAME,
    embedding_function=ollama_embeddings
)

# Cr√©e un retriever √† partir du vectorstore.
# Le retriever est responsable de la recherche des documents pertinents.
retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={
            "k": 5
        }
    )

# Initialise le mod√®le de chat Ollama
llm = ChatOllama(model=LLM_MODEL,
        temperature=0.1)

# --- 3. D√©finition du prompt RAG ---

# Le template du prompt pour le LLM.
# Il inclut le contexte r√©cup√©r√© et la question de l'utilisateur.
template = """Tu es un expert du menu du restaurant italien VAPIANO.

## INFORMATIONS IMPORTANTES
- R√©ponds UNIQUEMENT avec les informations du contexte fourni
- Sois PR√âCIS sur les compositions et prix
- Pour les allerg√®nes, traduis les num√©ros : 1=Gluten, 2=Crustac√©s, 3=≈íufs, 4=Poissons, 5=Arachides, 6=Soja, 7=Lait, 8=Fruits √† coque, 9=C√©leri, 10=Moutarde, 11=S√©same, 12=Sulfites, 13=Mollusques, 14=Lupin
- Si une information manque, dis "Information non disponible dans le menu"

## CONTEXTE DU MENU
{context}

## QUESTION
{question}

## R√âPONSE STRUCTUR√âE
Fournis une r√©ponse claire avec :
- üçΩÔ∏è Nom du plat
- ü•Ñ Composition/Ingr√©dients  
- üí∞ Prix
- ‚ö†Ô∏è Allerg√®nes (si applicable)
"""
prompt = ChatPromptTemplate.from_template(template)

# --- 4. Construction de la cha√Æne RAG avec LangChain Expression Language (LCEL) ---

# La cha√Æne RAG est construite en utilisant LCEL pour une meilleure lisibilit√© et modularit√©.
rag_chain = (
    {
        "context": retriever,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | StrOutputParser()
)

# --- 5. Boucle d'interaction ---

if __name__ == "__main__":
    print("\n--- Chatbot RAG avec LangChain ---")
    print("Posez des questions sur le document. Tapez 'exit' pour quitter.")

    while True:
        user_question = input("\nVous: ")
        if user_question.lower() == "exit":
            break

        print("Assistant: ...")
        # Invoque la cha√Æne RAG avec la question de l'utilisateur
        answer = rag_chain.invoke(user_question)
        print(f"\rAssistant: {answer}")
